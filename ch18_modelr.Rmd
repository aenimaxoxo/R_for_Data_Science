---
title: "Untitled"
author: "Michael Rose"
date: "February 19, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
options(na.action = na.warn)
```
## Model Basics with modelr

There are 2 parts to a model: 

1. You define a family of models that express a precise, but generic, pattern that you want to capture. For example, the pattern might be a straight line or a quadratic curve. You will express the model family as an equation like y = a_1 * x + a_2. 

2. Next you generate a fitted model by finding the moderl from the family that is closest to your data. This takes the generic model family and makes it specific, like y = 3 * x + 7

All models are wrong, but some are useful. - George Box

```{r}
ggplot(sim1, aes(x, y)) + geom_point()

# generate 250 linear models
 
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)

ggplot(sim1, aes(x, y)) + 
  geom_abline(
    aes(intercept = a1, slope = a2),
    data = models, alpha = 1/4
  ) + 
  geom_point()

# takes model parameters and the data as inputs and gives values predicted by the model as an output

model1 <- function(a, data){
  a[1] + data$x * a[2]
}

model1(c(7, 1.5), sim1)

# root mean squared deviation

measure_distance <- function(model, data){
  diff <- data$y - model1(model, data)
  sqrt(mean(diff ^ 2))
}

measure_distance(c(7, 1.5), sim1)

# use purrr to compute the distance for all the models defined previously
# we need a helper function because our model function expects a model as a numeric vector of length 2

sim1_dist <- function(a1, a2){
  measure_distance(c(a1, a2), sim1)
}

models <- models %>% mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

models

# overlay the 10 best models on the data. Best fitted models will get the brightest colors

ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, color = 'grey30') + 
  geom_abline(
    aes(intercept = a1, slope = a2, color = -dist),
    data = filter(models, rank(dist) <= 10)
  )

# we can also think of each model as an observation

ggplot(models, aes(a1, a2)) + 
  geom_point(
    data = filter(models, rank(dist) <= 10),
    size = 4, color = "red"
  ) + geom_point(aes(color = -dist))

# instead of trying lots of random models, we could be more systematic and generate an evenly spaced grid of points (this is called a grid search)

# make grid

grid <- expand.grid(
  a1 = seq(-5, 20, length = 25),
  a2 = seq(1, 3, length = 25)
) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

# plot

grid %>% ggplot(aes(a1, a2)) + 
  geom_point(
    data = filter(grid, rank(dist) <= 10),
    size = 4, color = "red"
  ) + 
  geom_point(aes(color = -dist))

# Overlaying the new 10 grid models onto the original data

ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, color = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, color = -dist),
    data = filter(grid, rank(dist) <= 10)
  )

# looks pretty good! 

# we can use a numerical minimization tool called newton raphson search. We can use that with optim()

best <- optim(c(0, 0), measure_distance, data = sim1)
best$par

ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, color = 'grey30') + 
  geom_abline(intercept = best$par[1], slope = best$par[2])

# R has a built in linear model function, lm

sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
```

```{r}
# One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the following simulated data, and visualize the results. Rerun a few times to generate different simulated datasets. What do you notice about the model? 

sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)

sim1a

sim1a_model <- lm(y ~ x, data = sim1a)

ggplot(sim1a, aes(x, y)) + 
  geom_point(size = 2, color = 'grey30') + 
  geom_abline(intercept = sim1a_model$coefficients[1], slope = sim1a_model$coefficients[2])

ggplot(sim1a, aes(x = x, y = y)) + 
  geom_point() + geom_smooth(method = "lm", se = FALSE)

# simulating several times using purrr and plotting with geom_smooth

simt <- function(i){
  tibble(
    x = rep(1:10, each = 3),
    y = x * 1.5 + 6 + rt(length(x), df = 2),
    .id = i
  )
}

sims <- map_df(1:12, simt)

ggplot(sims, aes(x = x, y = y)) + 
  geom_point() + 
  geom_smooth(method = "lm", color = "red") + 
  facet_wrap(~ .id, ncol = 4)

# What if we did the same thing with normal distributions? 

sim_norm <- function(i){
  tibble(
    x = rep(1:10, each = 3),
    y = x * 1.5 + 6 + rnorm(length(x)),
    .id = i
  )
}

simdf_norm <- map_df(1:12, sim_norm)

ggplot(simdf_norm, aes(x = x, y = y)) + 
  geom_point() + geom_smooth(method = 'lm', color = 'red') + 
  facet_wrap(~ .id, ncol = 4)

# There are not large outliers, and the slopes are more similar. The reason for this is that the Student's t-distribution, from which we sample with rt() has fatter tails than the normal distributions rnorm() which assigns larger probability to values further from the center of the distribution

tibble(
  x = seq(-5, 5, length.out = 100), 
  normal = dnorm(x),
  student_t = dt(x, df = 2)
  ) %>% 
  gather(distribution, density, -x) %>% 
  ggplot(aes(x, y = density, color = distribution)) + geom_line()

# for a normal distribution with mean zero and standard deviation one, the probability of being greater than 2 is
pnorm(2, lower.tail = FALSE)

# for a student's t-distribution with 2 degrees of freedom, it is more than 3 times higher
pt(2, df = 2, lower.tail = FALSE)

# One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance you could use mean-absolute distance:

measure_distance <- function(mod, data) {
  diff <- data$y - make_prediction(mod, data)
  mean(abs(diff))
}

# use optim() to fit this model to the previous simulated data and compare it to the linear model 

# to make the above work, we need to define a function make_prediction that takes an intercept and slope and returns the predictions 

make_prediction <- function(mod, data){
  mod[1] + mod[2] * data$x
}

best_sim1a <- optim(c(0, 0), measure_distance, data = sim1a)
best_sim1a$par

# using the sim1a data, the parameters that minimize the least squares objective function are 

measure_distance_ls <- function(mod, data){
  diff <- data$y - (mod[1] + mod[2] * data$x)
  sqrt(mean(diff ^ 2))
}

best_ls_sim1a <- optim(c(0, 0), measure_distance_ls, data = sim1a)
best_ls_sim1a$par

# in practice, you would not use an optim to fit this model, you would use an existing implementation. See the MASS package's rlm and lqs functions for more information and functions to fit robust and resistant linear models 

# One challenge with performing numerical optimization is that it's only guaranteed to find one local optima. What's the problem with optimizing a three parameter model like this ?

model3 <- function(a, data){
  a[1] + data$x * a[2] + a[3]
}

# the problem is that you have, for any values a[1] == a1, a[3] == a3, any other values of a[1] and a[3] where a[1] + a[3] == a1 + a3 will have the same fit

measure_distance_3 <- function(a, data){
  diff <- data$y - model3(a, data)
  sqrt(mean(diff^2))
}

# depending on our starting points, we can find different optimal values:

best3a <- optim(c(0, 0, 0), measure_distance_3, data = sim1)
best3a$par

best3b <- optim(c(0, 0, 1), measure_distance_3, data = sim1)
best3b$par

best3c <- optim(c(0, 0, 5), measure_distance_3, data = sim1)
best3c$par
```

### Visualizing Models 

```{r}
# Generate an evenly spaced grid of values that covers the region where our data lies. 

grid1 <- sim1 %>% data_grid(x)
grid1

# add predictions

grid <- grid1 %>% add_predictions(sim1_mod)
grid

ggplot(sim1, aes(x)) + 
  geom_point(aes(y = y)) + 
  geom_line(aes(y = pred), 
            data = grid,
            color = 'red',
            size = 1)

head(sim1)

```

#### Residuals

```{r}
sim1 <- sim1 %>% add_residuals(sim1_mod)
sim1

# frequency polygon to understand the spread of the residuals 

ggplot(sim1, aes(resid)) + 
  geom_freqpoly(binwidth = 0.5)

# recreate plot using the residuals instead of the original predictor

ggplot(sim1, aes(x, resid)) + 
  geom_ref_line(h = 0) + 
  geom_point()
```


```{r}
# instead of using lm() to fit a straight line, you can use loess() to fit a smooth curve. Repeat the process of model fitting, grid generation, predictions, and visualization on sim1 using loess() instead of lm(). How does the result compare to geom_smooth?

sim1_loess <- loess(y ~ x, data = sim1)
sim1_lm <- lm(y~x, data = sim1)

grid_loess <- sim1 %>% add_predictions(sim1_loess)

# add more residuals and predictions to the data

sim1 <- sim1 %>% 
  add_residuals(sim1_lm) %>% 
  add_predictions(sim1_lm) %>% 
  add_residuals(sim1_loess, var = 'resid_loess') %>% 
  add_predictions(sim1_loess, var = 'pred_loess')

# plot

plot_sim1_loess <- 
  ggplot(sim1, aes(x = x, y = y)) + 
  geom_point() + 
  geom_line(aes(x = x, y = pred), data = grid_loess, color = 'red')

plot_sim1_loess

# the predictions of loess are the same as the default method for geom_smooth because geom_smooth uses loess()

plot_sim1_loess + geom_smooth(method = 'loess', color = 'blue', se = FALSE, alpha = 0.2)

# we can plot the residuals from loess (red) and compare them to the residuals from lm (black). 

ggplot(sim1, aes(x = x)) + 
  geom_ref_line(h = 0) + 
  geom_point(aes(y = resid)) + 
  geom_point(aes(y = resid_loess), color = "red")

grid_loess
sim1

# add_predictions() is paired with gather_predictions() and spread_predictions() . How do these three functions differ? 

# add_predictions adds a single new column .pred to the input data
# spread_predictions adds one column for each model 
# gather_predictions adds two columns, .model and .pred and repeats the input rows for each model 

# gather_predictions and spread_predictions allow for adding predictions for multiple models at once. For example
sim1_mod <- lm(y ~ x, data = sim1)
grid <- sim1 %>% data_grid(x)
grid
# the function add_predictions adds only a single model at a time. To add two models: 
grid %>% 
  add_predictions(sim1_mod, var = 'pred_lm') %>% 
  add_predictions(sim1_loess, var = 'pred_loess')

# the function gather_predictions adds predictions from multiple models by stacking the results and adding a column with the model name
grid %>% gather_predictions(sim1_mod, sim1_loess)

# the function spread_predictions adds predictions from multiple models by adding multiple columns (postfixed with the model name) with predictions from each model
grid %>% spread_predictions(sim1_mod, sim1_loess) 

# the function spread_predictions is similar to the example which runs add_predictions for each model, and is equivalent to running spread after running gather_predictions

grid %>% gather_predictions(sim1_mod, sim1_loess) %>% spread(model, pred)
# What does geom_ref_line() do? What packages does it come from? Why is displaying a reference line in plots showing residuals useful and important? 

# the geom geom_ref_line() adds a reference line to the plot. It is equivalent to running geom_hline() or geom_vline() with default settings. Putting a reference line at zero for residuals is important because good models generally should have residuals centered at zero, with approximately the same variance (or distribution) over the support of x, and no correlation. A zero reference line makes it easier to judge these characteristics visually. 

# Why might you want to look at a frequency polygon of absolute residuals? What are the pros and cons compared to looking at the raw residuals ?

# Showing the absolute values of the residuals makes it easier to view the spread of the residuals. The model addumes that the residuals have mean zero, and using the absolute values of the residuals effectively doubles the number of residuals. 

sim1_mod <- lm(y ~ x, data = sim1)

sim1 <- sim1 %>% add_residuals(sim1_mod) 

ggplot(sim1, aes(x = abs(resid))) + 
  geom_freqpoly(binwidth = 0.5)

# the downside to using absolute values of residuals is that they throw away information about the sign, meaning that the frequency polygon can not show whether the model systematically over or under-estimates results

```

### Formulas and Model Families

```{r}
# The way that R defines formulas like y~x is by translating it into y = b0 + b1x. It does this by taking in a data frame and a formula and returing a tibble that defines the model equation. 
df <- tribble(
  ~y, ~x1, ~x2,
  4,    2,   5,
  5,    1,   6
)

model_matrix(df, y ~ x1)

# the way that R adds the intercept is by having a default column of 1s

# if we want to drop the 1s

model_matrix(df, y ~ x1 - 1)

model_matrix(df, y ~ x1 + x2)

```


### Categorical Variables

```{r}
df <- tribble(
  ~sex, ~response,
  'male', 1,
  'female', 2,
  'male', 1
)

model_matrix(df, response ~sex)

# look at some new data. Here x is categorical and y is continuous
ggplot(sim2) + geom_point(aes(x, y)) 

# fit a model to it
mod2 <- lm(y ~ x, data = sim2)

grid <- sim2 %>% data_grid(x) %>% add_predictions(mod2)

grid

# Effectively, a model with a categorical x will predict the mean value for each category (because the mean minimizes the mean squared distance)

# Overlay the predictions on top of the original data to clearly see means

ggplot(sim2, aes(x)) + 
  geom_point(aes(y = y)) + 
  geom_point(data = grid, 
             aes(y = pred),
             color = 'red',
             size = 4)
```

### Interactions (Continuous and Categorical)

```{r}
# What happens when we combine a continuous and categorical variable? 
ggplot(sim3, aes(x1, y)) + 
  geom_point(aes(color = x2))
sim3

# 2 possible models
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)

# when you use x1 + x2, x1 and x2 will estimate each effect independent from each other

# when you use x1 * x2, the model will fit the interaction variables, e.g. y = b0 + b1*x2 + b2*x2 + b3*x1*x2

# To fit these models, we need to 
# 1. We have two predictors, so we need to five data_grid() both variables. It finds all unique values of x1 and x2 ad then generates all combinations 
# 2. To generate predictions from both models simultaneously, we can use gather_predictions(), which adds each prediction as a row. The complement of gather_predictions() is spread_predictions() which adds each prediction to a new column

grid <- sim3 %>% 
  data_grid(x1, x2) %>% gather_predictions(mod1, mod2)
grid

ggplot(sim3, aes(x1, y, color = x2)) + 
  geom_point() + 
  geom_line(data = grid, aes(y = pred)) + 
  facet_wrap(~ model)

# see which model is better by looking at residuals

sim3 <- sim3 %>% gather_residuals(mod1, mod2)

ggplot(sim3, aes(x1, resid, color = x2)) + 
  geom_point() + facet_grid(model ~ x2) + geom_ref_line(h = 0, colour = 'black', size = 0.25)

```

### Interactions (Two Continuous)

```{r}
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

grid <- sim4 %>%
  data_grid(
    x1 = seq_range(x1, 5),
    x2 = seq_range(x2, 5)
  ) %>% 
  gather_predictions(mod1, mod2)

grid

# note the use of seq_range. Instead of using every unique value of x, hadley used a regularly spaced grid of five values between the minimum and maximum numbers.

# pretty = true generates a pretty sequence that looks nice to the human eye
seq_range(c(0.0123, 0.923423), n = 5)
# vs the pretty one
seq_range(c(0.0123, 0.923423), n = 5, pretty = TRUE)

# trim = 0.1 will trim off 10% of the tail values. This is useful if the variable has a long tailed distribution and you want to focus on generating values near the center
x1 <- rcauchy(100)
seq_range(x1, n = 5)
seq_range(x1, n=5, trim = 0.1)
seq_range(x1, n = 5, trim = 0.25)
seq_range(x1, n = 5, trim = 0.50)

# expand is the opposite of trim
x2 <- c(0, 1)
seq_range(x2, n = 5)
seq_range(x2, n = 5, expand = 0.1)
seq_range(x2, n = 5, expand = 0.25)
seq_range(x2, n = 5, expand = 0.5)

# we have 2 continuous predictors, so the model is like a 3d surface

ggplot(grid, aes(x1, x2)) + 
  geom_tile(aes(fill = pred)) + 
  facet_wrap(~ model)

# they don't look very different, but thats because our eyes aren't good at differentiating between similar shades of color. Lets look at the surface from the side as opposed to the top 

ggplot(grid, aes(x1, pred, color = x2, group = x2)) + 
  geom_line() + 
  facet_wrap(~model)

ggplot(grid, aes(x2, pred, color = x1, group = x1)) +
  geom_line() + 
  facet_wrap(~model)

```

### Transformations

```{r}
# You can perform transformations inside the model formula
# e.g. log(y) ~ sqrt(x1) + x2 is transformed to y = b0 + b1*x1*sqrt(x) + b2*x2

# If your transformation involves +, *, ^, or - you'll need to wrap it in I() so R doesn't treat it like part of the model specification. 
# e.g. y ~ x + I(x^2) translates to y = b0 + b1*x + b2*x^2
# if you forget the I() and specify y ~ x^2 + x, R computes y ~ x*x + x where x*x is the interaction of x with itself which is just x 

# you can use model_matrix to see exactly what your lm() is fitting
df <- tribble(
  ~y, ~x,
  1, 1, 
  2, 2,
  3, 3
)

model_matrix(df, y ~ x^2 + x)

model_matrix(df, y ~ I(x^2) + x)

# polynomial fit
model_matrix(df, y ~ poly(x, 2))

# major problem with poly: outside of the range of data, they rapidly shoot towards + or - infinity
# alternative is splines::ns() 

library(splines)

model_matrix(df, y ~ ns(x, 2))

# what that looks like when we try to approximate a nonlinear function
sim5 <- tibble(
  x = seq(0, 3.5*pi, length = 50),
  y = 4 * sin(x) + rnorm(length(x))
)

ggplot(sim5, aes(x, y)) + 
  geom_point()

# fitting 6 models to the data

mod1 <- lm(y ~ ns(x, 1), data = sim5)
mod2 <- lm(y ~ ns(x, 2), data = sim5)
mod3 <- lm(y ~ ns(x, 3), data = sim5)
mod4 <- lm(y ~ ns(x, 4), data = sim5)
mod5 <- lm(y ~ ns(x, 5), data = sim5)
mod6 <- lm(y ~ ns(x, 6), data = sim5)

grid <- sim5 %>% 
  data_grid(x = seq_range(x, n = 50, expand = 0.1)) %>% 
  gather_predictions(mod1, mod2, mod3, mod4, mod5, mod6, .pred = 'y')

ggplot(sim5, aes(x, y)) + 
  geom_point() + 
  geom_line(data = grid, color = 'red') + 
  facet_wrap(~model)

```

```{r}

# What happens if you repeat the analysis of sim2 using a model without an intercept? What happens to the model equation? What happens to the predictions? 

# to run a model without an intercept, add -1 or +0 to the right side of the formula

mod2a <- lm(y ~ x - 1, data = sim2)
mod2 <- lm(y ~ x, data = sim2)

grid <- sim2 %>% 
  data_grid(x) %>% 
  spread_predictions(mod2, mod2a)

grid

# the predictions are exactly the same 

# Use model_matrix to explore the equations generated for the models I fit to sim3 and sim4. Why is * a good sorthand for interaction? 

# for x1 * x2, when x2 is a categorical variable the command produces indicator variables x2b, x2c, x2d, and variables x1:x2b, x1:x2c, and x1:x2d which are the products of the x1 and x2* variables

x3 <- model_matrix(y ~ x1 * x2, data = sim3)
x3

# We can confirm that the variables x1:x2b is the product of x1 and x2b:

all(x3[['x1:x2b']] == (x3[['x1']] * x3[['x2b']]))

# and similarly for x1:x2c and x2c and x1:x2d and x2d

all(x3[['x1:x2c']] == (x3[['x1']] * x3[['x2c']]))
all(x3[['x1:x2d']] == (x3[['x1']] * x3[['x2d']]))

# for x1 * x2 where both x1 and x2 are continuous variables, model_matrix() creates variables x1, x2, and x1:x2
x4 <- model_matrix(y ~ x1*x2, data = sim4)
x4

# confirming that x1:x2 is a product of x1 and x2

all(x4[['x1:x2']] == x4[['x1']] * x4[['x2']])

# The asterisk is a good shorthand for an interaction since an interaction between x1 and x2 includes terms for x1, x2, and the product of x1 and x2

# Using the basic principles, convert the formulas in the following two models into functions (hint: start by converting the categorical variable into 0-1 variables)
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)

model_matrix_mod1 <- function(.data){
  mutate(.data, 
         'x2b' = as.numeric(x2 == 'b'),
         'x2c' = as.numeric(x2 == 'c'),
         'x2d' = as.numeric(x2 == 'd'),
         'x1:x2b' = x1 * x2b,
         'x1:x2c' = x1 * x2c,
         'x1:x2d' = x1 * x2d
         ) %>% 
    select(x1, x2b, x2c, x2d, 'x1:x2b', 'x1:x2c', 'x1:x2d')
}

model_matrix_mod1(sim3)
mod1

model_matrix_mod2 <- function(.data) {
  mutate(.data, 'x1:x2' = x1 * x2) %>% 
    select(x1, x2, 'x1:x2')
}

model_matrix_mod2(sim4)

# a more general function for mod1

model_matrix_mod1 <- function(x1, x2){
  out <- tibble(x1 = x1)
  # find levels of x2
  x2 <- as.factor(x2)
  x2lvls <- levels(x2)
  # create an indicator variable for each level
  for (lvl in x2lvls[2:nlevels(x2)]){
    out[[str_c('x1:x2', lvl)]] <- (x2 == lvl) * x1
  }
  out
}

model_matrix_mod2 <- function(x1, x2){
  out <- tibble(x1 = x1, 
                x2 = x2,
                'x1:x2' = x1 * x2)
}

# for sim4, which of mod1 and mod2 is better? 
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

# add the residuals to the sim4 data
sim4_mods <- gather_residuals(sim4, mod1, mod2)

# frequency plots of both the residuals
ggplot(sim4_mods, aes(x = resid, color = model)) + 
  geom_freqpoly(binwidth = 0.5) + 
  geom_rug()

# absolute value of the residuals
ggplot(sim4_mods, aes(x = abs(resid), color = model)) + 
  geom_freqpoly(binwidth = 0.5) + 
  geom_rug()

# There isn't much of a difference, but model 2 seems to work a bit better 

# show the standard deviation of the models

sim4_mods %>% 
  group_by(model) %>% 
  summarize(resid = sd(resid))
```

### Missing Values

```{r}
df <- tribble(
  ~x, ~y, 
  1, 2.2,
  2, NA,
  3, 3.5,
  4, 8.3, 
  NA, 10
)

mod <- lm(y ~ x, data = df)

# to supress the warning
mod <- lm(y ~ x, data = df, na.action = na.exclude)


```

